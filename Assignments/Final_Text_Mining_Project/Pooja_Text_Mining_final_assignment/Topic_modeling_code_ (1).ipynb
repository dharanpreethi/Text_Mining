{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpTYlab0QRxx",
        "outputId": "6fca6c38-d3cd-400e-e739-a2b8630b9da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'ramayana_chapters_with_numbers.csv' has been saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import csv\n",
        "\n",
        "# Step 1: Read the 'ramayana.txt' file\n",
        "with open('ramayana.txt', 'r', encoding='utf-8') as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "# Step 2: Define chapter titles with numbers\n",
        "chapter_titles = [\n",
        "    \"1. The Conception\", \"2. Sage Viswamitra\", \"3. Trisanku\", \"4. Rama Leaves Home\", \"5. Rama Slays The Monsters\",\n",
        "    \"6. Sita\", \"7. Bhagiratha And The Story Of Ganga\", \"8. Ahalya\", \"9. Rama Wins Sita's Hand\", \"10. Parasurama's Discomfiture\",\n",
        "    \"11. Festive Preparations\", \"12. Manthara's Evil Counsel\", \"13. Kaikeyi Succumbs\", \"14. Wife Or Demon?\", \"15. Behold A Wonder!\",\n",
        "    \"16. Storm And Calm\", \"17. Sita's Resolve\", \"18. To The Forest\", \"19. Alone By Themselves\", \"20. Chitrakuta\", \"21. A Mother's Grief\",\n",
        "    \"22. Idle Sport And Terrible Result\", \"23. Last Moments\", \"24. Bharata Arrives\", \"25. Intrigue wasted\", \"26. Bharata Suspected\",\n",
        "    \"27. The Brothers Meet\", \"28. Bharata Becomes Rama's Deputy\", \"29. Viradha's End\", \"30. Ten Years Pass\", \"31. The Surpanakha Episode\",\n",
        "    \"32. Kamban's Surpanakha\", \"33. Khara And His Army Liquidated\", \"34. The Path Of Ruin\", \"35. The Golden Stag\", \"36. The Good Bird Jatayu\",\n",
        "    \"37. Closely Guarded\", \"38. Rama Disconsolate\", \"39. A Second Father Dies\", \"40. Left Eyelids Throb\", \"41. He Sees Her Jewels\",\n",
        "    \"42. Sugriva's Doubts Cleared\", \"43. The Slaying Of Vali\", \"44. Tara's Grief\", \"45. Anger And Reconciliation\", \"46. The Search Begins\",\n",
        "    \"47. Son Of Vayu\", \"48. The Search In Lanka\", \"49. Sita In The Asoka Park\", \"50. Ravana's Solicitation\", \"51. First Among The Astute\",\n",
        "    \"52. Sita Comforted\", \"53. Sita And Hanuman\", \"54. Inviting Battle\", \"55. The Terrible Envoy\", \"56. Hanuman Bound\", \"57. Lanka In Flames\",\n",
        "    \"58. A Carnival\", \"59. The Tidings Conveyed\", \"60. The Army Moves Forward\", \"61. Anxiety In Lanka\", \"62. Ravana Calls A Council Again\",\n",
        "    \"63. Vibhishana\", \"64. The Vanara's Doubt\", \"65. Doctrine Of Surrender And Grace\", \"66. The Great Causeway\", \"67. The Battle Begins\",\n",
        "    \"68. Sita's Joy\", \"69. Serpent Darts\", \"70. Ravana's Defeat\", \"71. The Giant Is Roused\", \"72. Is This Narayana Himself?\", \"73. The Death Of Indrajit\",\n",
        "    \"74. End Of Ravana\", \"75. The End\", \"76. Epilogue\"\n",
        "]\n",
        "\n",
        "# Step 3: Regular expression to split text by chapter titles with numbers (case insensitive)\n",
        "pattern = r\"(\" + \"|\".join([re.escape(chapter) for chapter in chapter_titles]) + r\")\"\n",
        "sections = re.split(pattern, raw_text, flags=re.IGNORECASE)\n",
        "\n",
        "# Step 4: Pair up the chapter titles with their corresponding text\n",
        "chapter_contents = []\n",
        "for i in range(1, len(sections), 2):\n",
        "    title = sections[i].strip()\n",
        "    content = sections[i + 1].strip() if i + 1 < len(sections) else \"\"\n",
        "    chapter_contents.append([title, content])\n",
        "\n",
        "# Step 5: Save the data into a CSV file\n",
        "with open('ramayana_chapters_with_numbers.csv', 'w', encoding='utf-8', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"Chapter Title\", \"Content\"])  # Column headers\n",
        "    writer.writerows(chapter_contents)\n",
        "\n",
        "print(\"CSV file 'ramayana_chapters_with_numbers.csv' has been saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Step 1: Download necessary NLTK data\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9k5zYA_PQ5uP",
        "outputId": "2cda59f8-7559-4640-fece-74296516a424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Load the Ramayana data\n",
        "df = pd.read_csv(\"ramayana_chapters_with_numbers.csv\")\n",
        "\n",
        "# Assuming the text is in a column named 'Content' (adjust if needed)\n",
        "texts = df['Content'].dropna().astype(str)\n",
        "\n",
        "# Vectorize the text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Function to display topics\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
        "        keywords = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
        "        print(\"Keywords:\", \" | \".join(keywords))\n",
        "\n",
        "# Run topic modeling\n",
        "n_topics = 10\n",
        "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=100)\n",
        "lda_model.fit(X)\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Display the topics\n",
        "display_topics(lda_model, feature_names, no_top_words=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJIlzdtqTNdM",
        "outputId": "a55feb8f-0d65-4b6d-b3a3-ebd4075fa079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topic 1:\n",
            "Keywords: zeal | ploughed | drop | pastime | encouraging | nurses | entirety | merrily | maze | yogi\n",
            "\n",
            "Topic 2:\n",
            "Keywords: rama | sita | ravana | hanuman | said | king | lakshmana | like | forest | sugriva\n",
            "\n",
            "Topic 3:\n",
            "Keywords: zeal | ploughed | drop | pastime | encouraging | nurses | entirety | merrily | maze | yogi\n",
            "\n",
            "Topic 4:\n",
            "Keywords: zeal | ploughed | drop | pastime | encouraging | nurses | entirety | merrily | maze | yogi\n",
            "\n",
            "Topic 5:\n",
            "Keywords: ahalya | dravidian | entangled | partial | falsely | evoke | distinction | portrayals | engage | eunuch\n",
            "\n",
            "Topic 6:\n",
            "Keywords: trisanku | bhagiratha | amsuman | chandala | patala | sumati | kapila | exposed | maze | merrily\n",
            "\n",
            "Topic 7:\n",
            "Keywords: zeal | ploughed | drop | pastime | encouraging | nurses | entirety | merrily | maze | yogi\n",
            "\n",
            "Topic 8:\n",
            "Keywords: zeal | ploughed | drop | pastime | encouraging | nurses | entirety | merrily | maze | yogi\n",
            "\n",
            "Topic 9:\n",
            "Keywords: zeal | ploughed | drop | pastime | encouraging | nurses | entirety | merrily | maze | yogi\n",
            "\n",
            "Topic 10:\n",
            "Keywords: zeal | ploughed | drop | pastime | encouraging | nurses | entirety | merrily | maze | yogi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your chapters data (replace with your file if needed)\n",
        "chapters_df = pd.read_csv(\"ramayana_chapters_with_numbers.csv\")\n",
        "\n",
        "# List of female characters\n",
        "female_characters = [\"Sita\", \"Kaushalya\", \"Sumitra\", \"Kaikeyi\", \"Urmila\",\n",
        "                     \"Shurpanakha\", \"Mandodari\", \"Shanta\", \"Tataka\", \"Shabari\",\n",
        "                     \"Manthara\", \"Tara\", \"Ahalya\", \"Chandrabhaga\", \"Shrutakirti\",\n",
        "                     \"Sunayana\", \"Vershini\", \"Romapada\"]\n",
        "\n",
        "# Lowercase version for matching\n",
        "female_characters_lower = [name.lower() for name in female_characters]\n",
        "\n",
        "# Function to check if any female character is mentioned in the content\n",
        "def mentions_female_character(text):\n",
        "    text_lower = str(text).lower()  # Convert to lowercase and handle NaNs\n",
        "    return any(name in text_lower for name in female_characters_lower)\n",
        "\n",
        "# Apply filtering\n",
        "female_mentions_df = chapters_df[chapters_df['Content'].apply(mentions_female_character)]\n",
        "\n",
        "# Save filtered chapters to a new CSV\n",
        "female_mentions_df.to_csv(\"female_character_chapters.csv\", index=False)\n",
        "\n",
        "print(\"✅ Done! Saved chapters mentioning female characters to 'female_character_chapters.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp0A9E8xThUo",
        "outputId": "eca4d21a-84b0-420d-c5c3-01a34943120e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done! Saved chapters mentioning female characters to 'female_character_chapters.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load filtered chapters (only those mentioning female characters)\n",
        "df1 = pd.read_csv(\"female_character_chapters.csv\")\n",
        "\n",
        "# Preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(str(text).lower())\n",
        "    return ' '.join([\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in tokens\n",
        "        if word.isalpha() and word not in stop_words and len(word) > 2\n",
        "    ])\n",
        "\n",
        "df1['processed'] = df1['Content'].apply(preprocess)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_df=0.85, min_df=1)\n",
        "X = vectorizer.fit_transform(df1['processed'])\n",
        "\n",
        "# LDA Topic Modeling\n",
        "n_topics = 10 # You can tune this\n",
        "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=1000)\n",
        "lda_model.fit(X)\n",
        "\n",
        "# Display Unique Topics\n",
        "def display_unique_topics(model, feature_names, no_top_words=10):\n",
        "    unique_topic_sets = set()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        keywords = tuple([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
        "        if keywords not in unique_topic_sets:\n",
        "            unique_topic_sets.add(keywords)\n",
        "            print(f\"\\n🔹 Topic {len(unique_topic_sets)}:\")\n",
        "            print(\"Keywords:\", \" | \".join(keywords))\n",
        "\n",
        "# Get feature names and display unique topics\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "display_unique_topics(lda_model, feature_names, no_top_words=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp7YjYr4B-Kr",
        "outputId": "a2a0bdb6-f095-4386-a724-c2cc65c9b195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Topic 1:\n",
            "Keywords: menaka | zeal | maniac | entirety | encouraging | sastras | saras | artfully | emerge | emerald\n",
            "\n",
            "🔹 Topic 2:\n",
            "Keywords: manthara | wreak | trusty | hurriedly | hypocritical | hobbled | tended | intimacy | foresee | insane\n",
            "\n",
            "🔹 Topic 3:\n",
            "Keywords: ravana | hanuman | forest | sugriva | bharata | shall | brother | army | lanka | word\n",
            "\n",
            "🔹 Topic 4:\n",
            "Keywords: bhagiratha | amsuman | patala | yajnavalkya | zeal | joyfully | tangled | retrieve | ransack | respected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load filtered chapters (only those mentioning female characters)\n",
        "df1 = pd.read_csv(\"female_character_chapters.csv\")\n",
        "\n",
        "# Preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(str(text).lower())\n",
        "    return ' '.join([\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in tokens\n",
        "        if word.isalpha() and word not in stop_words and len(word) > 2\n",
        "    ])\n",
        "\n",
        "df1['processed'] = df1['Content'].apply(preprocess)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_df=0.85, min_df=1)\n",
        "X = vectorizer.fit_transform(df1['processed'])\n",
        "\n",
        "# LDA Topic Modeling\n",
        "n_topics = 11 # You can tune this\n",
        "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=1000)\n",
        "lda_model.fit(X)\n",
        "\n",
        "# Display Unique Topics\n",
        "def display_unique_topics(model, feature_names, no_top_words=10):\n",
        "    unique_topic_sets = set()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        keywords = tuple([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
        "        if keywords not in unique_topic_sets:\n",
        "            unique_topic_sets.add(keywords)\n",
        "            print(f\"\\n🔹 Topic {len(unique_topic_sets)}:\")\n",
        "            print(\"Keywords:\", \" | \".join(keywords))\n",
        "\n",
        "# Get feature names and display unique topics\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "display_unique_topics(lda_model, feature_names, no_top_words=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZu_mWbfBzaJ",
        "outputId": "a6e7f80d-5282-4d98-a1d0-58bbd2c0201a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Topic 1:\n",
            "Keywords: parrying | habitation | frighten | instill | slumbering | souvenir | damage | cracking | unaccountable | bodyguard\n",
            "\n",
            "🔹 Topic 2:\n",
            "Keywords: dadhimukha | parayana | furnishing | fatiguing | sheltered | keeper | recrossing | assaulted | greateful | declined\n",
            "\n",
            "🔹 Topic 3:\n",
            "Keywords: ravana | hanuman | forest | sugriva | bharata | shall | brother | army | lanka | word\n",
            "\n",
            "🔹 Topic 4:\n",
            "Keywords: bhagiratha | amsuman | bower | patala | sumati | kapila | rod | whirling | aksha | habitation\n",
            "\n",
            "🔹 Topic 5:\n",
            "Keywords: parasurama | bamboo | subsist | beehive | vie | abstemiously | unspoilt | twig | topographical | interference\n",
            "\n",
            "🔹 Topic 6:\n",
            "Keywords: ahalya | purana | lapse | entangled | untenanted | solicitation | reveled | frequent | evoke | eunuch\n",
            "\n",
            "🔹 Topic 7:\n",
            "Keywords: uttara | appreciated | legend | seed | gandhiji | parasurama | tyagaraja | behaves | rebel | drama\n",
            "\n",
            "🔹 Topic 8:\n",
            "Keywords: mahabali | tataka | conference | bowl | invitee | strictly | yagas | insolent | surrendering | distributed\n",
            "\n",
            "🔹 Topic 9:\n",
            "Keywords: simile | unregarded | intolerably | lake | insanity | piety | thrill | slander | tossed | bard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3lmh1N2FJj6U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}